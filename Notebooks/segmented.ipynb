{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Segmentation Automatique des Articles Scientifiques**"],"metadata":{"id":"-xfIsWcrWpNp"}},{"cell_type":"markdown","source":["##**Introduction**"],"metadata":{"id":"y62kAIPlW2CC"}},{"cell_type":"markdown","source":["Ce notebook a pour objectif de segmenter des articles scientifiques en paragraphes distincts afin de préparer les données pour des tâches ultérieures de traitement du langage naturel (NLP). La segmentation est une étape clé pour extraire des informations pertinentes et assurer la qualité des données avant l’analyse."],"metadata":{"id":"pqH2TKKwW7cW"}},{"cell_type":"markdown","source":["___________________"],"metadata":{"id":"hF4rOvKoW-1G"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tb2c9gLF1daO","executionInfo":{"status":"ok","timestamp":1727196779935,"user_tz":-60,"elapsed":2700,"user":{"displayName":"Mohamed Nassih","userId":"07780722299263513580"}},"outputId":"c43f89a0-71e3-4337-887c-93e9d74f2766"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Monter Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Sujet1NLP3D"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ah0DsJxr1oB8","executionInfo":{"status":"ok","timestamp":1727196779936,"user_tz":-60,"elapsed":13,"user":{"displayName":"Mohamed Nassih","userId":"07780722299263513580"}},"outputId":"896dbd23-7766-41e5-e6fb-7651df48a221"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Sujet1NLP3D\n"]}]},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nxud0vEPPIDt","executionInfo":{"status":"ok","timestamp":1727196790494,"user_tz":-60,"elapsed":10568,"user":{"displayName":"Mohamed Nassih","userId":"07780722299263513580"}},"outputId":"394da9df-3a57-42b5-ac9d-533b583537d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"]}]},{"cell_type":"markdown","source":["###**1. Importation des Bibliothèques Nécessaires**"],"metadata":{"id":"JnJm7LAxXH6i"}},{"cell_type":"markdown","source":["Charger les bibliothèques qui seront utilisées pour la segmentation et la gestion des données."],"metadata":{"id":"tbwg-ieTXTUF"}},{"cell_type":"code","source":["import os\n","import spacy\n","import gspread\n","from googleapiclient.discovery import build\n","from oauth2client.service_account import ServiceAccountCredentials\n","import re\n","from googleapiclient.http import MediaIoBaseDownload\n","import tiktoken"],"metadata":{"id":"hQuDZPJ51oLd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ces bibliothèques sont indispensables pour manipuler le contenu des articles, gérer les autorisations Google Drive, et traiter le texte (avec Spacy pour le NLP)"],"metadata":{"id":"K0n8ifdrXdSu"}},{"cell_type":"code","source":["# Initialisation de Spacy\n","nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"LRBZg19x18O0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**2. Authentification avec Google Drive et Google Sheets**"],"metadata":{"id":"wXt0o3YxXhKX"}},{"cell_type":"markdown","source":["Se connecter à Google Drive et Google Sheets pour accéder aux articles stockés sous forme de fichiers."],"metadata":{"id":"GROr9kS1Xkoa"}},{"cell_type":"code","source":["# Authentification avec l'API Google Drive\n","creds = ServiceAccountCredentials.from_json_keyfile_name('tools/custom-casing-434908-n5-4d3c56433812.json', [\"https://www.googleapis.com/auth/drive\"])\n","drive_service = build('drive', 'v3', credentials=creds)"],"metadata":{"id":"R66PWZ-_9eA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Connectez-vous à Google Sheets avec vos informations d'identification\n","client = gspread.authorize(creds)"],"metadata":{"id":"b2gKLLJa1oQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ouvrez votre fichier Excel qui contient les articles\n","sheet = client.open('arxiv_data').sheet1"],"metadata":{"id":"gDVGZjxu1oT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Récupérer les données de l'Excel\n","data = sheet.get_all_records()"],"metadata":{"id":"nTeWmuhk_xBV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cette section établit une connexion avec Google Drive pour télécharger les fichiers et Google Sheets pour récupérer les métadonnées des articles."],"metadata":{"id":"Y4-v6x3WX0GP"}},{"cell_type":"markdown","source":["###**3. Téléchargement et Lecture des Fichiers à Partir de Google Drive**"],"metadata":{"id":"A6ML8rK4YB1c"}},{"cell_type":"markdown","source":["Télécharger les fichiers texte des articles directement depuis Google Drive pour les traiter localement."],"metadata":{"id":"xoJ50U7OYLd-"}},{"cell_type":"code","source":["output_folder  = 'new_segmented_files'"],"metadata":{"id":"8K9DpqW0JeIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fonction pour extraire l'ID du fichier à partir du lien Google Drive\n","def extract_file_id(drive_link):\n","    match = re.search(r'/d/([a-zA-Z0-9_-]+)', drive_link)\n","    return match.group(1) if match else None"],"metadata":{"id":"s9vFL7zK_xHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fonction pour télécharger le fichier texte depuis Google Drive\n","def download_file(file_id, file_name):\n","    request = drive_service.files().get_media(fileId=file_id)\n","    file_path = f'/content/{file_name}.txt'\n","    with open(file_path, 'wb') as f:\n","        downloader = MediaIoBaseDownload(f, request)\n","        done = False\n","        while not done:\n","            status, done = downloader.next_chunk()\n","    return file_path"],"metadata":{"id":"4u6umJ34_xNU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cette fonction permet de télécharger les fichiers PDF/texte contenant les articles, en les sauvegardant localement pour les étapes suivantes de segmentation."],"metadata":{"id":"49lpFn8lYW1V"}},{"cell_type":"code","source":["# Fonction pour compter le nombre de tokens dans un texte\n","def count_tokens(text, model=\"gpt-4\"):\n","    encoder = tiktoken.encoding_for_model(model)\n","    tokens = encoder.encode(text)\n","    return len(tokens)"],"metadata":{"id":"7y0AK-_XO2bx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_section_digit(text, prev_text, max_section_tokens=12, model=\"gpt-4\", previous_section=None):\n","    \"\"\"\n","    Detects if a line is a section based on the following conditions:\n","    - The line must contain fewer than max_section_tokens (default 8).\n","    - The line contains only words (no numbers or symbols).\n","    - The preceding line (prev_text) must contain a valid section number in sequential order.\n","    - The section number should not be above 9.9.\n","    \"\"\"\n","    num_tokens = count_tokens(text, model)\n","\n","    # Condition 1: The token count must be within the limit\n","    if num_tokens > max_section_tokens:\n","        return False\n","\n","    # Condition 2: The current line must contain only words (no digits, symbols)\n","    digit_match = re.match(r'^([a-zA-Z\\s]+)$', text.strip())\n","    if not digit_match:\n","        return False\n","\n","    # Condition 3: The previous line must contain a section number in proper order\n","    match = re.match(r'^(\\d+(\\.\\d+)*)$', prev_text.strip())\n","    if not match:\n","        return False\n","\n","    current_section = match.group(1)\n","\n","    # Split the section number into parts\n","    current_nums = list(map(int, current_section.split('.')))\n","\n","    # Condition 4: Check that the section number is not above 9.9\n","    if len(current_nums) > 2 or current_nums[0] > 9 or (len(current_nums) == 2 and current_nums[1] > 9):\n","        return False\n","\n","    # Check if the current section is greater than the previous section\n","    if previous_section is not None:\n","        previous_nums = list(map(int, previous_section.split('.')))\n","\n","        if current_nums <= previous_nums:\n","            return False\n","\n","        # Ensure the current section is in sequence\n","        if current_nums[0] > previous_nums[0] + 1:\n","            return False\n","\n","    if re.search(r'[^\\w\\s[.-]]', text):\n","        return False\n","\n","    return current_section"],"metadata":{"id":"ZsCfdY5yIV1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fonction pour détecter une section précédée d'un numéro (ex: \"4.2. \") et suivie d'une phrase descriptive\n","def is_section_numbered_title(text, prev_text, max_section_tokens=12, model=\"gpt-4\", previous_section=None):\n","    \"\"\"\n","    Detects if a line is a section based on the format 'X.Y.', where:\n","    - X is a digit between 1 and 9.\n","    - Y is a digit between 0 and 9.\n","    - The line contains fewer than max_section_tokens (default 8).\n","    - The previous section must be less than or equal to the current section.\n","    \"\"\"\n","    num_tokens = count_tokens(text, model)\n","\n","    # Condition 1: The token count must be within the limit\n","    if num_tokens > max_section_tokens:\n","        return False\n","\n","    # Condition 2: The current line must start with a valid section number (1.1 to 9.9)\n","    match = re.match(r'^([1-9])\\.(\\d)\\s', text.strip())\n","    if not match:\n","        return False\n","\n","    current_section = match.group(0).strip()  # Capture the section part (e.g., \"4.2\")\n","\n","    # Condition 3: The section must be in proper order compared to the previous one\n","    if previous_section is not None:\n","        prev_nums = list(map(int, previous_section.split('.')))\n","        current_nums = list(map(int, current_section.split('.')))\n","\n","        # Check if the current section is greater than the previous section\n","        if current_nums <= prev_nums:\n","            return False\n","\n","        if re.search(r'[^\\w\\s[.-]]', text):\n","            return False\n","\n","    return current_section"],"metadata":{"id":"bmOwYLWXuytf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fonction pour détecter une section simple avec une phrase descriptive sans ponctuation ni chiffres\n","def is_section_simple_title(text, prev_text, max_section_tokens=12, model=\"gpt-4\"):\n","    \"\"\"\n","    Detects if a line is a section title that:\n","    - Contains only letters (no numbers or symbols).\n","    - Is alone on its line.\n","    - The previous line ends with a period (.).\n","    \"\"\"\n","    num_tokens = count_tokens(text, model)\n","\n","    # Condition 1: The token count must be within the limit\n","    if num_tokens > max_section_tokens:\n","        return False\n","\n","    # Condition 2: The previous line must end with a period (.)\n","    if not prev_text.strip().endswith('.'):\n","        return False\n","\n","    # Condition 3: The current line must only contain letters and spaces (no digits, no punctuation)\n","    if not re.match(r'^[a-zA-Z\\s]+$', text.strip()):\n","        return False\n","\n","    if re.search(r'[^\\w\\s[.-]]', text):\n","        return False\n","\n","    return True"],"metadata":{"id":"G52dI_HVvF5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fonction pour détecter si une ligne est une section basée sur des lettres\n","def is_section_letter(text, prev_text, max_section_tokens=12, model=\"gpt-4\", previous_section=None):\n","    \"\"\"\n","    Detects if a line is a section based on letters (a., b., c., etc.).\n","    The line must start with:\n","    - A lowercase letter followed by a period (e.g., 'a. Introduction').\n","    \"\"\"\n","    num_tokens = count_tokens(text, model)\n","\n","    # Condition 1: The token count must be within the limit\n","    if num_tokens > max_section_tokens:\n","        return False\n","\n","    if re.search(r'[^\\w\\s[.-]]', text):\n","        return False\n","\n","    # Condition 2: Match lowercase letters (a., b., c., etc.)\n","    letter_match = re.match(r'^([a-e])\\.\\s', text.strip())\n","\n","    if letter_match:\n","        current_letter = letter_match.group(1)\n","\n","        # Verify alphabetical order\n","        if previous_section is not None:\n","            prev_letter = previous_section.lower()\n","            if ord(current_letter) <= ord(prev_letter):\n","                return False\n","\n","        return current_letter\n","\n","    return False"],"metadata":{"id":"zKdSun4uPJ0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fonction pour détecter si une ligne est une section avec chiffres romains\n","def is_section_alpha_romain(text, prev_text, max_section_tokens=12, model=\"gpt-4\", previous_section=None):\n","    num_tokens = count_tokens(text, model)\n","    if num_tokens > max_section_tokens:\n","        return False\n","\n","    if re.search(r'[^\\w\\s[.-]]', text):\n","        return False\n","\n","    # Modifier pour capturer uniquement la partie avant le point\n","    roman_match = re.match(r'^([ivxlcdm]+)\\.\\s', text.strip(), re.IGNORECASE)\n","    if roman_match:\n","        current_section = roman_match.group(1).strip().lower()  # Prend seulement la partie avant le point\n","\n","        roman_to_int = {'i': 1, 'ii': 2, 'iii': 3, 'iv': 4, 'v': 5, 'vi': 6, 'vii': 7, 'viii': 8, 'ix': 9, 'x': 10}\n","        current_value = roman_to_int.get(current_section)\n","\n","        # Vérifier que current_value n'est pas None avant la comparaison\n","        if current_value is not None:\n","            if previous_section is not None and previous_section in roman_to_int:\n","                prev_value = roman_to_int[previous_section]\n","                if current_value <= prev_value:\n","                    return False\n","            return current_section  # Retourne la valeur de la section en chiffres romains (ex: 'i', 'ii', etc.)\n","\n","    return False"],"metadata":{"id":"7zqmx8ibcn4E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fonction pour diviser un paragraphe long en sous-parties\n","def split_long_paragraph(paragraph, max_tokens=300, token_threshold=250, model=\"gpt-4\"):\n","    tokens = count_tokens(paragraph, model)\n","    if tokens <= max_tokens:\n","        return [paragraph]  # Pas besoin de diviser\n","\n","    # Diviser le paragraphe d'abord par des points pour obtenir des phrases complètes\n","    sentences = paragraph.split('.')\n","    result = []\n","    current_segment = \"\"\n","\n","    # Diviser par des points\n","    for sentence in sentences:\n","        if count_tokens(current_segment + sentence, model) < max_tokens:\n","            current_segment += sentence + '.'\n","        else:\n","            # Si le segment actuel est trop long, diviser par d'autres ponctuations\n","            split_segment = split_by_punctuation(current_segment, max_tokens, token_threshold, model)\n","            result.extend(split_segment)\n","            current_segment = sentence + '.'\n","\n","    # Ajouter le dernier segment\n","    if current_segment:\n","        split_segment = split_by_punctuation(current_segment, max_tokens, token_threshold, model)\n","        result.extend(split_segment)\n","\n","    return result"],"metadata":{"id":"NRjaCHdgJovb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fonction pour diviser par des virgules, points-virgules, deux-points, etc.\n","def split_by_punctuation(text, max_tokens, token_threshold, model):\n","    \"\"\"Divise un texte par des virgules et autres ponctuations tout en respectant max_tokens et token_threshold.\"\"\"\n","    splitters = [',', ';', ':', '!', '?']\n","    result = []\n","    current_segment = \"\"\n","\n","    for splitter in splitters:\n","        if splitter in text:\n","            sub_segments = text.split(splitter)\n","            for part in sub_segments:\n","                if count_tokens(current_segment + part, model) < max_tokens:\n","                    current_segment += part + splitter\n","                else:\n","                    if count_tokens(current_segment, model) >= token_threshold:\n","                        result.append(current_segment.strip())\n","                    current_segment = part + splitter\n","\n","    # Vérifier le dernier segment\n","    if current_segment and count_tokens(current_segment, model) >= token_threshold:\n","        result.append(current_segment.strip())\n","\n","    return result"],"metadata":{"id":"bj5Y9shqJore"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**4. Détection et Segmentation des Paragraphes**"],"metadata":{"id":"ijxm-LLmYzrx"}},{"cell_type":"markdown","source":["Détecter les sections et paragraphes dans chaque article pour les structurer correctement."],"metadata":{"id":"uSPmzncRY-1B"}},{"cell_type":"code","source":["def structure_text(content, token_threshold=250, max_tokens=300, min_tokens=200, model=\"gpt-4\"):\n","    paragraphs = []\n","    current_paragraph = []\n","    lines = content.splitlines()\n","\n","    prev_line = \"\"\n","    previous_section_digit = None\n","    previous_section_letter = None\n","    previous_section_alpha_romain = None\n","    previous_section_numbered_title = None\n","\n","    section_count = {\n","        'digit': {},\n","        'letter': {},\n","        'alpha_romain': {},\n","        'numbered_title': {},\n","        'simple_title': {}\n","    }\n","\n","    for line in lines:\n","        if line.strip():  # Ignore empty lines\n","            section_digit = is_section_digit(line.strip(), prev_line, max_section_tokens=12,\n","                                             model=model, previous_section=previous_section_digit)\n","            section_numbered_title = is_section_numbered_title(line.strip(), prev_line, max_section_tokens=12,\n","                                                               model=model, previous_section=previous_section_numbered_title)\n","            section_letter = is_section_letter(line.strip(), prev_line, max_section_tokens=12,\n","                                               model=model, previous_section=previous_section_letter)\n","            section_alpha_romain = is_section_alpha_romain(line.strip(), prev_line, max_section_tokens=12,\n","                                                           model=model, previous_section=previous_section_alpha_romain)\n","            section_simple_title = is_section_simple_title(line.strip(), prev_line, max_section_tokens=12, model=model)\n","\n","            # Mise à jour des occurrences des sections\n","            if section_digit:\n","                section_count['digit'][section_digit] = section_count['digit'].get(section_digit, 0) + 1\n","                if section_count['digit'][section_digit] > 1:\n","                    section_digit = None\n","\n","            if section_numbered_title:\n","                section_count['numbered_title'][section_numbered_title] = section_count['numbered_title'].get(section_numbered_title, 0) + 1\n","                if section_count['numbered_title'][section_numbered_title] > 1:\n","                    section_numbered_title = None\n","\n","            if section_letter:\n","                section_count['letter'][section_letter] = section_count['letter'].get(section_letter, 0) + 1\n","                if section_count['letter'][section_letter] > 1:\n","                    section_letter = None\n","\n","            if section_alpha_romain:\n","                section_count['alpha_romain'][section_alpha_romain] = section_count['alpha_romain'].get(section_alpha_romain, 0) + 1\n","                if section_count['alpha_romain'][section_alpha_romain] > 1:\n","                    section_alpha_romain = None\n","\n","            if section_simple_title:\n","                section_count['simple_title'][section_simple_title] = section_count['simple_title'].get(section_simple_title, 0) + 1\n","                if section_count['simple_title'][section_simple_title] > 1:\n","                    section_simple_title = None\n","\n","            if section_digit or section_letter or section_alpha_romain or section_numbered_title or section_simple_title:\n","                if current_paragraph:\n","                    paragraph_text = \" \".join(current_paragraph).strip()\n","                    if count_tokens(paragraph_text, model) >= min_tokens:\n","                        paragraphs.append(paragraph_text)\n","                    current_paragraph = []\n","\n","                if section_digit:\n","                    previous_section_digit = section_digit\n","                if section_letter:\n","                    previous_section_letter = section_letter\n","                if section_alpha_romain:\n","                    previous_section_alpha_romain = section_alpha_romain\n","                if section_numbered_title:\n","                    previous_section_numbered_title = section_numbered_title\n","\n","                paragraphs.append('\\n' + line.upper().strip())\n","            else:\n","                current_paragraph.append(line.strip())\n","\n","            prev_line = line\n","\n","    if current_paragraph:\n","        paragraph_text = \" \".join(current_paragraph).strip()\n","        if count_tokens(paragraph_text, model) >= min_tokens:\n","            paragraphs.append(paragraph_text)\n","\n","    # 1. Remove paragraphs with mostly digits or punctuation\n","    filtered_paragraphs = []\n","    for paragraph in paragraphs:\n","        tokens = count_tokens(paragraph, model)\n","        if tokens > 0 and (count_tokens(re.sub(r'[^\\d\\s]', '', paragraph), model) / tokens) >= 0.4:\n","            continue\n","        filtered_paragraphs.append(paragraph)\n","\n","    paragraphs = filtered_paragraphs\n","\n","    # 2. Remove section titles with fewer than 5 tokens\n","    filtered_paragraphs = []\n","    for paragraph in paragraphs:\n","        if paragraph.startswith('\\n') and count_tokens(paragraph.strip(), model) < 5:\n","            continue\n","        filtered_paragraphs.append(paragraph)\n","\n","    return filtered_paragraphs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"hHt9ROEkDpXY","executionInfo":{"status":"ok","timestamp":1727196803934,"user_tz":-60,"elapsed":37,"user":{"displayName":"Mohamed Nassih","userId":"07780722299263513580"}},"outputId":"023a75cc-4f30-4b0f-d58e-e402962153a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n    # 1. Remove paragraphs after \"references\"\\n    references_found = False\\n    for i, paragraph in enumerate(paragraphs):\\n        if \\'references\\' in paragraph.lower():\\n            references_found = True\\n            paragraphs = paragraphs[:i]  # Keep only paragraphs before \"references\"\\n            break\\n\\n    if not references_found:\\n        paragraphs = paragraphs[:-10]  # Remove last 3 paragraphs if \"references\" not found\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["Cette fonction permet de structurer chaque texte en détectant les sections et paragraphes, en tenant compte du nombre de tokens et de certaines règles de ponctuation."],"metadata":{"id":"IW7cOeooZGaU"}},{"cell_type":"code","source":["# Processus de traitement du texte avec structure par section et division de paragraphes longs\n","def process_text_file(file_path, title, token_threshold=250, max_tokens=300, min_tokens=200, model=\"gpt-4\"):\n","    # Lecture complète du fichier\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        content = f.read()\n","\n","    # Structurer le texte avec sections et paragraphes\n","    paragraphs = structure_text(content, token_threshold, max_tokens, min_tokens, model)\n","\n","    # Sauvegarder avec le titre\n","    formatted_content = f\"{title}\\n\\n\"\n","    for paragraph in paragraphs:\n","        if is_section_digit(paragraph, \"\", model=model):  # Ligne seule considérée comme une section\n","            formatted_content += f\"{paragraph}\\n\"\n","        else:\n","            split_paragraphs = split_long_paragraph(paragraph, max_tokens, token_threshold, model)\n","            for p in split_paragraphs:\n","                formatted_content += f\"{p}\\n\\n\"\n","\n","    return formatted_content"],"metadata":{"id":"qrBZu7z8Joka"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**5. Sauvegarde des Fichiers Segmentés**"],"metadata":{"id":"Cn7IllXcZPgh"}},{"cell_type":"markdown","source":["Enregistrer chaque fichier d'article segmenté dans un nouveau fichier texte structuré."],"metadata":{"id":"oKVPPgJEZa_e"}},{"cell_type":"code","source":["# Processus complet de segmentation et filtrage pour chaque fichier\n","for record in data:\n","    drive_link = record['Content']  # Extraire le lien de partage\n","    title = record['Title']  # Récupérer le titre depuis l'Excel\n","    file_id = extract_file_id(drive_link)\n","\n","    if file_id:\n","        file_name = f\"article_{data.index(record) + 1}\"  # Nommer les fichiers en fonction de leur index\n","        file_path = download_file(file_id, file_name)\n","\n","        # Processus de structuration et de sauvegarde des paragraphes\n","        formatted_content = process_text_file(file_path, title, token_threshold=250, max_tokens=300, min_tokens=200, model=\"gpt-4\")\n","\n","        # Sauvegarde du fichier structuré et filtré\n","        output_file = os.path.join(output_folder, f\"{file_name}_segmented.txt\")\n","        with open(output_file, 'w', encoding='utf-8') as f_out:\n","            f_out.write(formatted_content)\n","\n","        print(f\"Fichier segmenté et filtré sauvegardé : {output_file}\")\n","    else:\n","        print(f\"Impossible d'extraire l'ID du fichier pour le lien : {drive_link}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUzUz8x7JohK","outputId":"389375da-056c-402c-c38c-dce7ed18637a","executionInfo":{"status":"ok","timestamp":1727197689179,"user_tz":-60,"elapsed":885276,"user":{"displayName":"Mohamed Nassih","userId":"07780722299263513580"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fichier segmenté et filtré sauvegardé : new_segmented_files/article_1_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_2_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_3_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_4_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_5_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_6_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_7_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_8_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_9_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_10_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_11_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_12_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_13_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_14_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_15_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_16_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_17_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_18_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_19_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_20_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_21_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_22_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_23_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_24_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_25_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_26_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_27_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_28_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_29_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_30_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_31_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_32_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_33_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_34_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_35_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_36_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_37_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_38_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_39_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_40_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_41_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_42_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_43_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_44_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_45_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_46_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_47_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_48_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_49_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_50_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_51_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_52_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_53_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_54_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_55_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_56_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_57_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_58_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_59_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_60_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_61_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_62_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_63_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_64_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_65_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_66_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_67_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_68_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_69_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_70_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_71_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_72_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_73_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_74_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_75_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_76_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_77_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_78_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_79_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_80_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_81_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_82_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_83_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_84_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_85_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_86_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_87_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_88_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_89_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_90_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_91_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_92_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_93_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_94_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_95_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_96_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_97_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_98_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_99_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_100_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_101_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_102_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_103_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_104_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_105_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_106_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_107_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_108_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_109_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_110_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_111_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_112_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_113_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_114_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_115_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_116_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_117_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_118_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_119_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_120_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_121_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_122_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_123_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_124_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_125_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_126_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_127_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_128_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_129_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_130_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_131_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_132_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_133_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_134_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_135_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_136_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_137_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_138_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_139_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_140_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_141_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_142_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_143_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_144_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_145_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_146_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_147_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_148_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_149_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_150_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_151_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_152_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_153_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_154_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_155_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_156_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_157_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_158_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_159_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_160_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_161_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_162_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_163_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_164_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_165_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_166_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_167_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_168_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_169_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_170_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_171_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_172_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_173_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_174_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_175_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_176_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_177_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_178_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_179_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_180_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_181_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_182_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_183_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_184_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_185_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_186_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_187_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_188_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_189_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_190_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_191_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_192_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_193_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_194_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_195_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_196_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_197_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_198_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_199_segmented.txt\n","Fichier segmenté et filtré sauvegardé : new_segmented_files/article_200_segmented.txt\n"]}]},{"cell_type":"markdown","source":["Après la segmentation, chaque fichier est sauvegardé dans un dossier spécifique. Le contenu est structuré et prêt pour une analyse future."],"metadata":{"id":"164ceikZZkBN"}},{"cell_type":"markdown","source":["_______________________"],"metadata":{"id":"7ajZjp18ZqIm"}},{"cell_type":"markdown","source":["##**Conclusion**"],"metadata":{"id":"sCIpIphpZwsJ"}},{"cell_type":"markdown","source":["Ce notebook permet d’automatiser la segmentation des articles scientifiques en paragraphes distincts, un processus essentiel pour garantir une extraction d’information précise et structurée lors des étapes suivantes du projet."],"metadata":{"id":"I9ExsZCvZ25L"}}]}
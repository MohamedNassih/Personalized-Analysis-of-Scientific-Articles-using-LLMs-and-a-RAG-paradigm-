# -*- coding: utf-8 -*-
"""search and recovery.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jj7JFST5iM33nD4zxeGkpIYmMrntbf4C
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Sujet1NLP3D

!pip install chromadb

import chromadb
import pandas as pd
import re
from transformers import DistilBertTokenizer, DistilBertModel

# Indiquer le chemin vers le répertoire de Chroma dans Google Drive
CHROMA_PATH = "chroma_db_persistent"

# Initialiser le client Chroma avec le répertoire persistant
client = chromadb.PersistentClient(path=CHROMA_PATH) # Set the persist directory here

# Create or get the collection, persist_directory is no longer needed.
collection = client.get_or_create_collection("Article_Embedding")

# Charger les modèles
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

def get_distilbert_embeddings(text):
    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy().reshape(-1)  # Reshape to 1D

# Fonction pour interroger Chroma et structurer les résultats dans un DataFrame
def extract_information_from_chroma(query, num_results=5):
    # Générer l'embedding pour la requête
    query_embedding = get_distilbert_embeddings(query)

    # Rechercher les documents les plus similaires dans Chroma
    results = collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=num_results
    )

    # Extraire les documents (sections) et métadonnées
    documents = results['documents']
    metadatas = results['metadatas']

    # Créer une liste pour stocker les résultats sous forme de dictionnaires
    extracted_data = []

    # Parcourir les documents et métadonnées pour lier chaque section à ses métadonnées
    for doc_idx, doc_sections in enumerate(documents):
        for section_idx, section in enumerate(doc_sections):
            metadata = metadatas[doc_idx][section_idx]

            # Ajouter la section, ses métadonnées et le texte à la liste
            extracted_data.append({
                'Section': section,  # Le texte de la section
                'Metadata': metadata  # Métadonnées correspondantes
            })

    # Créer un DataFrame avec les résultats extraits
    df_results = pd.DataFrame(extracted_data)
    return df_results

# Exemple d'utilisation
query = """
Advanced techniques for optimizing systolic engines in FPGAs,
including the use of DSP48E2 blocks in neural network architectures.
These techniques encompass weight prefetching in matrix systolic
engines to improve computational efficiency in embedded systems.
In particular, the focus is on improving performance in neural
network FPGA accelerators, such as Google's TPUv1 and Xilinx Vitis AI DPU,
which exploit specific architectures to maximize parallelization and energy efficiency.
Recent advances in multiplexing techniques in DSP48E2, ring accumulator design, and power
consumption reduction are also considered to provide state-of-the-art solutions for complex neural systems.
This approach also explores the integration of systolic engines in FPGA systems for neuromorphic applications,
such as neural network-based accelerators. spiking neurons, with key technical optimizations aimed at
reducing resource usage and improving overall computational performance.
"""
df = extract_information_from_chroma(query)
df

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity

# Assuming `query_embedding` is the embedding of the query
# and `paragraph_embeddings` is a list of embeddings of the 5 paragraphs

# Get the query embedding
query_embedding = get_distilbert_embeddings(query)

# Get paragraph embeddings (assuming 'Section' column contains the paragraphs)
paragraph_embeddings = [get_distilbert_embeddings(text) for text in df['Section'].tolist()]

embeddings = [query_embedding] + paragraph_embeddings  # Combine query and paragraphs embeddings

# Reduce dimensionality to 2D for visualization (you can use PCA or t-SNE)
def reduce_dimensions(embeddings, method='pca'):
    if method == 'pca':
        pca = PCA(n_components=2)
        reduced_embeddings = pca.fit_transform(embeddings)
    elif method == 'tsne':
        tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
        reduced_embeddings = tsne.fit_transform(embeddings)
    return reduced_embeddings

# Calculate cosine similarity between the query and each paragraph
def calculate_similarity(query_embedding, paragraph_embeddings):
    similarities = cosine_similarity([query_embedding], paragraph_embeddings)
    return similarities.flatten()

# Reduce dimensions of the embeddings
reduced_embeddings = reduce_dimensions(embeddings, method='pca')  # or 'tsne'

# Plot the embeddings
plt.figure(figsize=(8, 6))
plt.scatter(reduced_embeddings[0, 0], reduced_embeddings[0, 1], color='red', label='Query')
for i, (x, y) in enumerate(reduced_embeddings[1:], start=1):
    plt.scatter(x, y, color='blue', label=f'Paragraph {i}')
    plt.text(x + 0.01, y + 0.01, f'P{i}', fontsize=12)

plt.title('Embedding Visualization of Query and Extracted Paragraphs')
plt.legend()
plt.show()

# Calculate and print similarity scores
similarities = calculate_similarity(query_embedding, paragraph_embeddings)
for i, sim in enumerate(similarities, start=1):
    print(f'Similarity between query and paragraph {i}: {sim:.4f}')
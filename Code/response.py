# -*- coding: utf-8 -*-
"""response.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zesn4mLovfJw7c2sjKfrmBbFvAGvBxb_
"""

!pip install langchain chromadb langchain_community transformers requests

from langchain_community.embeddings.ollama import OllamaEmbeddings
from langchain.vectorstores.chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from langchain.vectorstores import Chroma
import chromadb
import pandas as pd
import re
import requests
from transformers import DistilBertTokenizer, DistilBertModel

# Indiquer le chemin vers le répertoire de Chroma dans Google Drive
CHROMA_PATH = "chroma_db_persistent"

# Initialiser le client Chroma avec le répertoire persistant
client = chromadb.PersistentClient(path=CHROMA_PATH) # Set the persist directory here

# Create or get the collection, persist_directory is no longer needed.
collection = client.get_or_create_collection("Article_Embedding")

# Charger les modèles
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

def get_distilbert_embeddings(text):
    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy().reshape(-1)  # Reshape to 1D

PROMPT_TEMPLATE = """
You are a chatbot assistant who answers scientific questions.
Your responses should be tailored based on the user's profile and in English.

User profile: {profile}

Answer the question based on the following context and take into account the user's profile:

{context}

---

Answer the question based on the above context and profile: {question}
"""

# Fonction pour extraire le contexte depuis Chroma
def extract_information_from_chroma(query, num_results=10):
    # Générer l'embedding pour la requête
    query_embedding = get_distilbert_embeddings(query)

    # Rechercher les documents les plus similaires dans Chroma
    results = collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=num_results
    )

    # Extraire les documents (sections)
    documents = results['documents']

    # Extraire le contexte
    context = ""
    for i in range(min(len(documents), num_results)):
        # Vérifier si le document est une liste de sections
        if isinstance(documents[i], list):
            # Concaténer toutes les sections du document
            context_text = " ".join(documents[i])
        else:
            context_text = documents[i]

        context += context_text + "\n\n"

    return context

# Fonction pour interagir avec l'API d'Ollama via Docker
def query_ollama_with_context(context, query_text, profile):
    # Préparer le prompt avec le contexte
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context, question=query_text, profile=profile)

    # Envoyer la requête à l'API locale d'Ollama (qui doit tourner via Docker)
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "llama3.1", "prompt": prompt},
        stream=True  # Active le streaming
    )

    # Initialiser une variable pour stocker la réponse complète
    full_response = ""

    # Traiter chaque ligne de la réponse (streaming)
    for line in response.iter_lines():
        if line:
            # Décoder chaque ligne en JSON
            partial_response = line.decode('utf-8')
            try:
                # Convertir en JSON
                json_data = requests.models.complexjson.loads(partial_response)
                # Concaténer la partie 'response' du JSON à la réponse complète
                full_response += json_data.get('response', '')
            except ValueError as e:
                print(f"Erreur lors de la conversion JSON: {e}")
                print(f"Ligne brute : {partial_response}")

    # Afficher la réponse complète
    return full_response

# Fonction principale qui combine ChromaDB et Ollama
def query_rag_with_context(query_text, profile):
    # Extraire le contexte depuis Chroma
    context = extract_information_from_chroma(query_text)

    # Interroger Ollama avec le contexte et la question
    response_text = query_ollama_with_context(context, query_text, profile)

    # Retourner la réponse générée
    return response_text

# Exemple d'utilisation
query = """
How can advanced techniques for optimizing systolic engines in FPGAs,
particularly through the use of DSP48E2 blocks in neural network architectures,
improve computational efficiency in embedded systems? Specifically,
how do techniques like weight prefetching in matrix systolic engines enhance
performance in FPGA accelerators such as Google's TPUv1 and Xilinx Vitis AI DPU,
which rely on specialized architectures to maximize parallelization and energy efficiency?
Additionally, what role do recent innovations in DSP48E2 multiplexing techniques,
ring accumulator design, and power consumption reduction play in providing state-of-the-art
solutions for complex neural systems? Lastly, how can these optimizations be applied
to integrate systolic engines in FPGA systems for neuromorphic applications,
including neural network-based accelerators and spiking neurons, with a focus on reducing
resource usage and improving overall computational performance?
"""

response = query_rag_with_context(query, "researcher")
print(response)
# -*- coding: utf-8 -*-
"""embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jGzHY-SPoloDIf-hk9A0OuVUbVkNU0Ti
"""

# Monter Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Sujet1NLP3D

!pip install gensim transformers sentence-transformers torch chromadb gspread oauth2client spacy tiktoken

!python -m spacy download en_core_web_sm

import os
import re
import gensim
import torch
import chromadb
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from oauth2client.service_account import ServiceAccountCredentials
import gspread
import time
import spacy
from chromadb.utils import embedding_functions
from transformers import DistilBertTokenizer, DistilBertModel

# Initialisation de Spacy
nlp = spacy.load("en_core_web_sm")

# Indiquer le chemin vers le répertoire de Chroma dans Google Drive
CHROMA_PATH = "chroma_db_persistent"

# Initialiser le client Chroma avec le répertoire persistant
client = chromadb.PersistentClient(path=CHROMA_PATH) # Set the persist directory here

# Create or get the collection, persist_directory is no longer needed.
collection = client.get_or_create_collection("Article_Embedding")

# Authentification avec l'API Google Drive
creds = ServiceAccountCredentials.from_json_keyfile_name('tools/custom-casing-434908-n5-4d3c56433812.json', ["https://www.googleapis.com/auth/drive"])
drive_service = build('drive', 'v3', credentials=creds)

# Connectez-vous à Google Sheets avec vos informations d'identification
client = gspread.authorize(creds)

# Ouvrez votre fichier Excel qui contient les articles
sheet = client.open('arxiv_data').sheet1

# Récupérer les données de l'Excel
data = sheet.get_all_records()

# Fonction pour extraire l'ID du fichier à partir du lien Google Drive
def extract_file_id(drive_link):
    match = re.search(r'/d/([a-zA-Z0-9_-]+)', drive_link)
    return match.group(1) if match else None

# Fonction pour télécharger le fichier texte depuis Google Drive
def download_file(file_id, file_name):
    request = drive_service.files().get_media(fileId=file_id)
    file_path = f'/content/{file_name}.txt'
    with open(file_path, 'wb') as f:
        downloader = MediaIoBaseDownload(f, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
    return file_path

# Charger les modèles
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

def get_distilbert_embeddings(text):
    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy().reshape(-1)  # Reshape to 1D

# Fonction pour segmenter un article en titres, sections, et paragraphes
def segment_article(content):
    """
    Segmente l'article en titre, sections et paragraphes.
    - Le titre est sur la première ligne.
    - Les sections sont séparées du texte précédent et suivant par 3 sauts de ligne.
    - Les paragraphes sont séparés par 2 sauts de ligne.
    """
    # Séparer le contenu en lignes
    lines = content.splitlines()

    # Extraire le titre (première ligne)
    title = lines[0].strip()

    # Reconstruire le texte sans le titre
    text = "\n".join(lines[1:]).strip()

    # Diviser d'abord en sections en utilisant 3 sauts de ligne
    sections = text.split("\n\n\n")

    segmented_text = []

    # Parcourir chaque section pour identifier les paragraphes
    for section in sections:
        section = section.strip()
        if section:
            # Diviser chaque section en paragraphes avec 2 sauts de ligne
            paragraphs = section.split("\n\n")
            paragraphs = [p.strip() for p in paragraphs if p.strip()]

            # Si la section commence par un titre, l'ajouter en majuscule
            if paragraphs[0].isupper():
                segmented_text.append(paragraphs[0])  # Ajouter la section en majuscule
                paragraphs = paragraphs[1:]  # Retirer le titre de la liste des paragraphes

            # Ajouter les paragraphes
            segmented_text.extend(paragraphs)

    return title, segmented_text

# Dossier contenant les fichiers segmentés
segmented_dir = 'new_segmented_files'

# Initialiser la liste pour stocker les contenus des fichiers
corpus = []
file_names = []

# Vérifier que le dossier existe et charger les articles
if os.path.exists(segmented_dir):
    for file_name in os.listdir(segmented_dir):
        if file_name.endswith(".txt"):
            file_path = os.path.join(segmented_dir, file_name)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                corpus.append(content)  # Ajouter le contenu de l'article à la liste
                file_names.append(file_name)  # Ajouter le nom du fichier
    print(f"{len(corpus)} articles ont été chargés depuis {segmented_dir}.")
else:
    print(f"Le dossier {segmented_dir} n'existe pas.")

# Utiliser uniquement DistilBERT pour générer et stocker les embeddings dans Chroma
embedding_func = get_distilbert_embeddings

# Stocker les embeddings pour chaque paragraphe/section
for i, content in enumerate(corpus):
    # Segmenter l'article en titre, sections et paragraphes
    title, segmented_text = segment_article(content)

    # Générer les embeddings pour chaque paragraphe/section
    for j, paragraph in enumerate(segmented_text):
        embeddings = embedding_func(paragraph)

        # Convertir l'embedding en liste
        embeddings = embeddings.tolist()

        # Ajouter les embeddings pour chaque paragraphe dans Chroma
        collection.add(
            ids=[f"{i}_{j}"],  # Utiliser l'index de l'article et du paragraphe
            documents=[paragraph],  # Stocker chaque paragraphe/section
            embeddings=[embeddings],  # Embedding du paragraphe/section
            metadatas=[{"file_name": file_names[i], "paragraph_index": j}]  # Stocker le nom de fichier et l'indice du paragraphe
        )
print(f"Les embeddings pour chaque paragraphe/section ont été stockés dans Chroma à {CHROMA_PATH}.")

# Interroger Chroma avec une requête exemple
query = "Deep learning techniques for image segmentation in computer vision"

# Use the same embedding function that was used to generate embeddings for the collection
# Assuming GloVe was the best model and its embedding function is 'get_glove_embeddings':
query_embedding = embedding_func(query)  # Instead of get_sentence_embeddings

# Rechercher les articles les plus similaires dans la collection
results = collection.query(
    query_embeddings=[query_embedding.tolist()],
    n_results=5  # Nombre de résultats à retourner
)

# Afficher les résultats (documents et métadonnées)
for i, result in enumerate(results['documents']):
    print(f"Article {i+1}: {result}")
    print(f"Métadonnées associées : {results['metadatas'][i]}")